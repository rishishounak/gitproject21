{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZegFCp_Kcws"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/yolov7oc2'"
      ],
      "metadata": {
        "id": "SR2KwDCgKhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "HWa-99kUKipd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotbbox(imgpath,labpath):\n",
        "  import matplotlib.pyplot as plt\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  img1=plt.imread(imgpath)\n",
        "  f1=open(labpath,'r')\n",
        "  lab1=f1.readlines()\n",
        "  shape1=img1.shape\n",
        "  print(shape1)\n",
        "  str1=lab1[0].split()\n",
        "  print(lab1)\n",
        "  x1=shape1[1]*float(str1[1])\n",
        "  x2=shape1[0]*float(str1[2])\n",
        "  x3=shape1[1]*float(str1[3])\n",
        "  x4=shape1[0]*float(str1[4])\n",
        "  color = (255, 0, 0)\n",
        "  \n",
        "  # Line thickness of 2 pxint(x1+(x3/2))  int(x2-(x4/2))\n",
        "  thickness = 2\n",
        "  ib1=cv2.rectangle(img1,(int(x1-(x3/2)),int(x2-(x4/2))),(int(x1+(x3/2)),int(x2+(x4/2))) , color, thickness)\n",
        "  # ib1=cv2.rectangle(img1, (0,0), (5,5), color, thickness)\n",
        "  cv2_imshow(ib1)\n",
        "  return ib1\n",
        "  "
      ],
      "metadata": {
        "id": "TCih77-2KkW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationsAndGradients:\n",
        "    \"\"\" Class for extracting activations and\n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layers, reshape_transform):\n",
        "        self.model = model\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.handles = []\n",
        "        for target_layer in target_layers:\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_activation))\n",
        "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
        "            # we don't use backward hook to record gradients.\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_gradient))\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        activation = output\n",
        "\n",
        "        if self.reshape_transform is not None:\n",
        "            activation = self.reshape_transform(activation)\n",
        "        self.activations.append(activation.cpu().detach())\n",
        "\n",
        "    def save_gradient(self, module, input, output):\n",
        "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
        "            # You can only register hooks on tensor requires grad.\n",
        "            return\n",
        "\n",
        "        # Gradients are computed in reverse order\n",
        "        def _store_grad(grad):\n",
        "            if self.reshape_transform is not None:\n",
        "                grad = self.reshape_transform(grad)\n",
        "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
        "\n",
        "        output.register_hook(_store_grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        return self.model(x)\n",
        "\n",
        "    def release(self):\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n",
        "\n"
      ],
      "metadata": {
        "id": "vIopcwYUKmPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getpaths():\n",
        "  imgpaths=[]\n",
        "  labpaths=[]\n",
        "  path1='custom_dataset/images/val'\n",
        "  path2='custom_dataset/labels/val'\n",
        "  for x in os.listdir(path1):\n",
        "    imgpaths.append(path1+'/'+x)\n",
        "  for x in os.listdir(path2):\n",
        "    labpaths.append(path2+'/'+x)\n",
        "  return imgpaths,labpaths"
      ],
      "metadata": {
        "id": "GgSh_22cKpX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgwithbox1=plotbbox('/content/drive/MyDrive/yolov7oc2/custom_dataset/images/val/sanjaykumarimage 0.jpeg','/content/drive/MyDrive/yolov7oc2/custom_dataset/labels/val/sanjaykumarimage 0.txt')"
      ],
      "metadata": {
        "id": "NII9mU6yKrGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.experimental import attempt_load\n",
        "from utils.torch_utils import TracedModel\n",
        "\n",
        "wpath='/content/drive/MyDrive/yolov7oc2/runs/train/yolov7-custom/weights/best.pt'\n",
        "\n",
        "import torch\n",
        "\n",
        "device=torch.device('cpu')\n",
        "\n",
        "model = attempt_load(wpath, map_location=device)\n"
      ],
      "metadata": {
        "id": "CRMr0K9UKtOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layers=[]\n",
        "n1=0\n",
        "for x in range(len(model.model)):\n",
        "  layers.append(model.model[x])"
      ],
      "metadata": {
        "id": "arwNp5qJKvvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1=ActivationsAndGradients(model,target_layers=layers[0:104],reshape_transform=None)"
      ],
      "metadata": {
        "id": "IEWemi-kKxXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rkdyL-Z5KzIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gresults=['sashirajimage 1.jpeg','soorsinghimage 2.jpeg','surajmalikimage 1.jpeg','ramalingamimage 2.jpeg']"
      ],
      "metadata": {
        "id": "TZVQLAXILX37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for picr1 in gresults:\n",
        "  import cv2\n",
        "  im0 = cv2.imread('/content/drive/MyDrive/yolov7oc2/custom_dataset/images/val/'+picr1)\n",
        "\n",
        "  img=letterbox(im0,640,32)[0]\n",
        "\n",
        "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "  img = np.ascontiguousarray(img)\n",
        "\n",
        "  import torch\n",
        "  device=torch.device('cpu')\n",
        "  img = torch.from_numpy(img).to(device)\n",
        "  img = img.float()  # uint8 to fp16/32\n",
        "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "  if img.ndimension() == 3:\n",
        "      img = img.unsqueeze(0)\n",
        "\n",
        "  img.requires_grad=True\n",
        "\n",
        "  outputs1=y1(img)\n",
        "\n",
        "  # from utils.general import apply_classifier\n",
        "  # from utils.general import non_max_suppression\n",
        "  # pred = non_max_suppression(outputs1[0],0.25,0.45)\n",
        "        \n",
        "  # pred = apply_classifier(pred, modelc, img, im0)\n",
        "\n",
        "  outputs1[0][0][:,4].max().backward()\n",
        "\n",
        "\n",
        "  backprop1=torch.clone(img.grad.detach())\n",
        "  backprop1=torch.relu(backprop1)\n",
        "  backprop1=torch.squeeze(backprop1)\n",
        "\n",
        "\n",
        "  r1=torch.rand((backprop1.shape[1],backprop1.shape[2],3))\n",
        "  r1[:,:,0]=backprop1[0,:,:]\n",
        "  r1[:,:,1]=backprop1[1,:,:]\n",
        "  r1[:,:,2]=backprop1[2,:,:]\n",
        "\n",
        "\n",
        "  backprop1=r1\n",
        "  backprop1=(backprop1-backprop1.min())/backprop1.max()\n",
        "\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  cv2_imshow(backprop1.numpy()*255)\n",
        "\n",
        "\n",
        "  # plt.imsave('/content/drive/MyDrive/yolov7oc2/finoutputs2/guidbackprop'+picr1,(backprop1.numpy()*255).astype(np.uint8))\n",
        "  backpropimg=(backprop1.numpy()*255).astype(np.uint8).copy()\n",
        "\n",
        "  weights1=torch.mean(y1.gradients[101],axis=(2,3))\n",
        "  gradcam=torch.clone(y1.activations[101].detach())\n",
        "  for x in range(gradcam.shape[1]):\n",
        "    gradcam[0,x,:,:]=weights1[0][x].detach()*gradcam[0,x,:,:]\n",
        "\n",
        "  gradcamf=torch.sum(gradcam[0].detach(),axis=0)\n",
        "  gradcamf=torch.relu(gradcamf)\n",
        "\n",
        "  import torchvision\n",
        "  transform1=torchvision.transforms.functional.resize(gradcamf.view([1,gradcamf.shape[0],gradcamf.shape[1]]),(img.shape[2],img.shape[3]))\n",
        "\n",
        "  gradcam=transform1.view([img.shape[2],img.shape[3]])\n",
        "\n",
        "  gradcam=(gradcam-gradcam.min())/gradcam.max()\n",
        "\n",
        "  cv2_imshow(gradcam.detach().numpy()*255)\n",
        "\n",
        "  # plt.imsave('/content/drive/MyDrive/yolov7oc2/finoutputs2/gradcammap'+picr1,gradcam.detach().numpy()*255)\n",
        "\n",
        "  gradcamimg=(gradcam.detach().numpy()*255).copy()\n",
        "\n",
        "  for x in range(3):\n",
        "    backprop1[:,:,0]=gradcam*backprop1[:,:,0]\n",
        "    backprop1[:,:,1]=gradcam*backprop1[:,:,1]\n",
        "    backprop1[:,:,2]=gradcam*backprop1[:,:,2]\n",
        "\n",
        "  # plt.imsave('/content/drive/MyDrive/yolov7oc2/finoutputs2/guidedgradcam'+picr1,(backprop1.detach().numpy()*255*255).astype(np.uint8))\n",
        "  guidedgradcam=(backprop1.detach().numpy()*255*255).astype(np.uint8).copy()\n",
        "\n",
        "  imgforgrad=torch.clone(img.detach()).squeeze()\n",
        "  for x in range(3):\n",
        "    imgforgrad[x,:,:]=imgforgrad[x,:,:]*gradcam*255*255\n",
        "\n",
        "  r2=torch.rand((img.shape[2],img.shape[3],3))\n",
        "  r2[:,:,0]=imgforgrad[0,:,:]\n",
        "  r2[:,:,1]=imgforgrad[1,:,:]\n",
        "  r2[:,:,2]=imgforgrad[2,:,:]\n",
        "\n",
        "\n",
        "  # plt.imsave('/content/drive/MyDrive/yolov7oc2/finoutputs2/gradcamimp'+picr1,r2.detach().numpy()/(255*255))\n",
        "  gradcamsuperimpose=(r2.detach().numpy()/(255*255)).copy()\n",
        "\n",
        "  rx=torch.rand((img.shape[2],img.shape[3],3))\n",
        "  rx[:,:,0]=img[0][0]\n",
        "  rx[:,:,1]=img[0][1]\n",
        "  rx[:,:,2]=img[0][2]\n",
        "  # plt.imsave('/content/drive/MyDrive/yolov7oc2/finoutputs2/'+picr1,(rx.detach().numpy()))\n",
        "  originimg=rx.detach().numpy().copy()\n",
        "  dirx='/content/drive/MyDrive/yolov7oc2/runs/detect/exp7'\n",
        "  detectimg=cv2.imread(dirx+'/'+picr1)\n",
        "  \n",
        "\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "    \n",
        "  # setting values to rows and column variables\n",
        "  rows = 2\n",
        "  columns = 3\n",
        "    \n",
        "  # reading images\n",
        "  Image1 = originimg\n",
        "  Image2 = detectimg\n",
        "  Image3 = backpropimg\n",
        "  Image4 = gradcamimg\n",
        "  Image5 = gradcamsuperimpose\n",
        "  Image6 = guidedgradcam\n",
        "    \n",
        "  # Adds a subplot at the 1st position\n",
        "  fig.add_subplot(rows, columns, 1)\n",
        "    \n",
        "  # showing image\n",
        "  plt.imshow(Image1)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"origin\")\n",
        "    \n",
        "  # Adds a subplot at the 2nd position\n",
        "  fig.add_subplot(rows, columns, 2)\n",
        "    \n",
        "  # showing image\n",
        "  plt.imshow(Image2)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"detectpic\")\n",
        "    \n",
        "  # Adds a subplot at the 3rd position\n",
        "  fig.add_subplot(rows, columns, 3)\n",
        "    \n",
        "  # showing image\n",
        "  plt.imshow(Image3)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"backproppic\")\n",
        "    \n",
        "  # Adds a subplot at the 4th position\n",
        "  fig.add_subplot(rows, columns, 4)\n",
        "    \n",
        "  # showing image\n",
        "  plt.imshow(Image4)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"gradcampic\")\n",
        "\n",
        "  fig.add_subplot(rows, columns, 5)\n",
        "    \n",
        "  # showing image\n",
        "  plt.imshow(Image5)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"gradcamsuperimpose\")\n",
        "\n",
        "  fig.add_subplot(rows, columns, 6)\n",
        "    \n",
        "  # showing image\n",
        "  plt.imshow(Image6)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"guidedgradcam\")\n",
        "\n",
        "  plt.savefig('/content/drive/MyDrive/yolov7oc2/finoutputs3/'+picr1)"
      ],
      "metadata": {
        "id": "PJi3Tk8AK-Ri"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}